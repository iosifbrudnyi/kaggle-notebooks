{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48795a0-52cf-49a2-b6fe-0100ac1d8b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import logging  \n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eabe105-09c6-4f41-8c1f-927f16e32b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158309</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I'm back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158310</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>You see, class, my Lyme disease turned out to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158311</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>Psy-cho-so-ma-tic.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158312</th>\n",
       "      <td>Ralph Wiggum</td>\n",
       "      <td>Does that mean you were crazy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158313</th>\n",
       "      <td>JANEY</td>\n",
       "      <td>No, that means she was faking it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158314 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             raw_character_text  \\\n",
       "0                   Miss Hoover   \n",
       "1                  Lisa Simpson   \n",
       "2                   Miss Hoover   \n",
       "3                  Lisa Simpson   \n",
       "4       Edna Krabappel-Flanders   \n",
       "...                         ...   \n",
       "158309              Miss Hoover   \n",
       "158310              Miss Hoover   \n",
       "158311              Miss Hoover   \n",
       "158312             Ralph Wiggum   \n",
       "158313                    JANEY   \n",
       "\n",
       "                                             spoken_words  \n",
       "0       No, actually, it was a little of both. Sometim...  \n",
       "1                                  Where's Mr. Bergstrom?  \n",
       "2       I don't know. Although I'd sure like to talk t...  \n",
       "3                              That life is worth living.  \n",
       "4       The polls will be open from now until the end ...  \n",
       "...                                                   ...  \n",
       "158309                                          I'm back.  \n",
       "158310  You see, class, my Lyme disease turned out to ...  \n",
       "158311                                 Psy-cho-so-ma-tic.  \n",
       "158312                     Does that mean you were crazy?  \n",
       "158313                  No, that means she was faking it.  \n",
       "\n",
       "[158314 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"simpsons_dataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d37056f-9ba7-43ca-9aa0-84fd3d8d3972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "raw_character_text    17814\n",
       "spoken_words          26459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a484d04e-bc7c-4564-a519-2117b6e75ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b08d6c7-f3ae-4f27-a76f-d8a9ae869968",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\", \"parser\"])\n",
    "\n",
    "def cleaning(doc):\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "035bb35e-ac2a-4dc2-b221-b5d5b834acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing non-alpabetic characters\n",
    "brief_cleaning = (re.sub( \"[^A-Za-z]\", \" \", str(row) ).lower() for row in df[\"spoken_words\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f51da7f7-b4fb-41de-b392-b86ca3f5c509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55 s, sys: 1.33 s, total: 56.3 s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_process=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f659b1-1405-4761-9c20-2e7600af37d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101440, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = pd.DataFrame({\"clean\" : txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9350ff8-1e8b-40ff-adba-7cfa52590c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:32:37: collecting all words and their counts\n",
      "INFO - 18:32:37: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 18:32:37: PROGRESS: at sentence #10000, processed 62487 words and 46791 word types\n",
      "INFO - 18:32:37: PROGRESS: at sentence #20000, processed 129901 words and 88432 word types\n",
      "INFO - 18:32:37: PROGRESS: at sentence #30000, processed 195229 words and 124610 word types\n",
      "INFO - 18:32:37: PROGRESS: at sentence #40000, processed 255163 words and 156068 word types\n",
      "INFO - 18:32:37: PROGRESS: at sentence #50000, processed 311914 words and 184134 word types\n",
      "INFO - 18:32:37: PROGRESS: at sentence #60000, processed 375510 words and 216077 word types\n",
      "INFO - 18:32:37: PROGRESS: at sentence #70000, processed 438052 words and 246129 word types\n",
      "INFO - 18:32:38: PROGRESS: at sentence #80000, processed 501664 words and 276109 word types\n",
      "INFO - 18:32:38: PROGRESS: at sentence #90000, processed 564332 words and 305369 word types\n",
      "INFO - 18:32:38: PROGRESS: at sentence #100000, processed 628771 words and 332991 word types\n",
      "INFO - 18:32:38: collected 336799 token types (unigram + bigrams) from a corpus of 638246 words and 101440 sentences\n",
      "INFO - 18:32:38: merged Phrases<336799 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 18:32:38: Phrases lifecycle event {'msg': 'built Phrases<336799 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.63s', 'datetime': '2023-11-21T18:32:38.161243', 'gensim': '4.3.2', 'python': '3.9.16 (main, Dec  7 2022, 01:12:08) \\n[GCC 11.3.0]', 'platform': 'Linux-5.19.0-45-generic-x86_64-with-glibc2.35', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "sent = [row.split() for row in df_clean['clean']]\n",
    "phrases = Phrases(sent, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa9310e6-3160-4e21-a96f-64acd568321e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:32:38: exporting phrases from Phrases<336799 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n",
      "INFO - 18:32:38: FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<138 phrases, min_count=30, threshold=10.0> from Phrases<336799 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000> in 0.53s', 'datetime': '2023-11-21T18:32:38.700013', 'gensim': '4.3.2', 'python': '3.9.16 (main, Dec  7 2022, 01:12:08) \\n[GCC 11.3.0]', 'platform': 'Linux-5.19.0-45-generic-x86_64-with-glibc2.35', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cb1226a-be21-4243-98e1-d61ab30a59a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30035"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd2c6fe0-589c-4a6e-9503-dc30c4879972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 'm', 'oh', 'don_t', 'll', 'like', 'know', 'hey', 'think', 'right']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1581d804-4243-4283-814a-37a6b7eedd83",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccb76235-f918-4afa-8f93-5555d56a43dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98e29b7d-0581-4ab6-9317-75c687fa62b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da4de3db-7f66-4552-a35c-9b6045daef37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:32:39: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.03>', 'datetime': '2023-11-21T18:32:39.279153', 'gensim': '4.3.2', 'python': '3.9.16 (main, Dec  7 2022, 01:12:08) \\n[GCC 11.3.0]', 'platform': 'Linux-5.19.0-45-generic-x86_64-with-glibc2.35', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.34 ms, sys: 66 Âµs, total: 1.41 ms\n",
      "Wall time: 1.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_model = Word2Vec(\n",
    "    min_count=20,\n",
    "    window=2,\n",
    "    vector_size=300,\n",
    "    sample=6e-5,\n",
    "    alpha=0.03,\n",
    "    min_alpha=0.0007,\n",
    "    negative=20,\n",
    "    workers=cores-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0acefc0-37e5-4212-aff9-ddcdf40123e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:32:39: collecting all words and their counts\n",
      "INFO - 18:32:39: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 18:32:39: PROGRESS: at sentence #10000, processed 59382 words, keeping 8610 word types\n",
      "INFO - 18:32:39: PROGRESS: at sentence #20000, processed 123594 words, keeping 13154 word types\n",
      "INFO - 18:32:39: PROGRESS: at sentence #30000, processed 185852 words, keeping 16270 word types\n",
      "INFO - 18:32:39: PROGRESS: at sentence #40000, processed 243123 words, keeping 18744 word types\n",
      "INFO - 18:32:39: PROGRESS: at sentence #50000, processed 297306 words, keeping 20812 word types\n",
      "INFO - 18:32:39: PROGRESS: at sentence #60000, processed 358133 words, keeping 23012 word types\n",
      "INFO - 18:32:39: PROGRESS: at sentence #70000, processed 417950 words, keeping 24938 word types\n",
      "INFO - 18:32:39: PROGRESS: at sentence #80000, processed 478855 words, keeping 26741 word types\n",
      "INFO - 18:32:39: PROGRESS: at sentence #90000, processed 538870 words, keeping 28427 word types\n",
      "INFO - 18:32:39: PROGRESS: at sentence #100000, processed 600275 words, keeping 29842 word types\n",
      "INFO - 18:32:39: collected 30035 word types from a corpus of 609265 raw words and 101440 sentences\n",
      "INFO - 18:32:39: Creating a fresh vocabulary\n",
      "INFO - 18:32:39: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 3426 unique words (11.41% of original 30035, drops 26609)', 'datetime': '2023-11-21T18:32:39.885540', 'gensim': '4.3.2', 'python': '3.9.16 (main, Dec  7 2022, 01:12:08) \\n[GCC 11.3.0]', 'platform': 'Linux-5.19.0-45-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "INFO - 18:32:39: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 522070 word corpus (85.69% of original 609265, drops 87195)', 'datetime': '2023-11-21T18:32:39.886209', 'gensim': '4.3.2', 'python': '3.9.16 (main, Dec  7 2022, 01:12:08) \\n[GCC 11.3.0]', 'platform': 'Linux-5.19.0-45-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "INFO - 18:32:39: deleting the raw counts dictionary of 30035 items\n",
      "INFO - 18:32:39: sample=6e-05 downsamples 1088 most-common words\n",
      "INFO - 18:32:39: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 226087.37438200708 word corpus (43.3%% of prior 522070)', 'datetime': '2023-11-21T18:32:39.902523', 'gensim': '4.3.2', 'python': '3.9.16 (main, Dec  7 2022, 01:12:08) \\n[GCC 11.3.0]', 'platform': 'Linux-5.19.0-45-generic-x86_64-with-glibc2.35', 'event': 'prepare_vocab'}\n",
      "INFO - 18:32:39: estimated required memory for 3426 words and 300 dimensions: 9935400 bytes\n",
      "INFO - 18:32:39: resetting layer weights\n",
      "INFO - 18:32:39: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-11-21T18:32:39.934267', 'gensim': '4.3.2', 'python': '3.9.16 (main, Dec  7 2022, 01:12:08) \\n[GCC 11.3.0]', 'platform': 'Linux-5.19.0-45-generic-x86_64-with-glibc2.35', 'event': 'build_vocab'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 636 ms, sys: 16.2 ms, total: 652 ms\n",
      "Wall time: 649 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca315f1d-7cd5-47ab-a670-c1455ee5c03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:32:39: Word2Vec lifecycle event {'msg': 'training model with 7 workers on 3426 vocabulary and 300 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2 shrink_windows=True', 'datetime': '2023-11-21T18:32:39.939937', 'gensim': '4.3.2', 'python': '3.9.16 (main, Dec  7 2022, 01:12:08) \\n[GCC 11.3.0]', 'platform': 'Linux-5.19.0-45-generic-x86_64-with-glibc2.35', 'event': 'train'}\n",
      "INFO - 18:32:40: EPOCH 0: training on 609265 raw words (226420 effective words) took 0.9s, 260101 effective words/s\n",
      "INFO - 18:32:41: EPOCH 1 - PROGRESS: at 79.02% examples, 177478 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 18:32:42: EPOCH 1: training on 609265 raw words (226020 effective words) took 1.1s, 212795 effective words/s\n",
      "INFO - 18:32:42: EPOCH 2: training on 609265 raw words (226113 effective words) took 1.0s, 236957 effective words/s\n",
      "INFO - 18:32:44: EPOCH 3 - PROGRESS: at 87.23% examples, 195063 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 18:32:44: EPOCH 3: training on 609265 raw words (226069 effective words) took 1.1s, 206915 effective words/s\n",
      "INFO - 18:32:45: EPOCH 4 - PROGRESS: at 93.72% examples, 210581 words/s, in_qsize 4, out_qsize 1\n",
      "INFO - 18:32:45: EPOCH 4: training on 609265 raw words (226160 effective words) took 1.0s, 222318 effective words/s\n",
      "INFO - 18:32:46: EPOCH 5 - PROGRESS: at 96.92% examples, 218516 words/s, in_qsize 2, out_qsize 1\n",
      "INFO - 18:32:46: EPOCH 5: training on 609265 raw words (225758 effective words) took 1.0s, 222619 effective words/s\n",
      "INFO - 18:32:47: EPOCH 6: training on 609265 raw words (225773 effective words) took 1.0s, 232795 effective words/s\n",
      "INFO - 18:32:48: EPOCH 7: training on 609265 raw words (226307 effective words) took 1.0s, 234710 effective words/s\n",
      "INFO - 18:32:49: EPOCH 8 - PROGRESS: at 83.88% examples, 180557 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:32:49: EPOCH 8: training on 609265 raw words (225797 effective words) took 1.2s, 195347 effective words/s\n",
      "INFO - 18:32:50: EPOCH 9 - PROGRESS: at 69.29% examples, 151487 words/s, in_qsize 13, out_qsize 0\n",
      "INFO - 18:32:50: EPOCH 9: training on 609265 raw words (226195 effective words) took 1.2s, 194986 effective words/s\n",
      "INFO - 18:32:51: EPOCH 10 - PROGRESS: at 85.52% examples, 190755 words/s, in_qsize 9, out_qsize 0\n",
      "INFO - 18:32:51: EPOCH 10: training on 609265 raw words (226113 effective words) took 1.1s, 210260 effective words/s\n",
      "INFO - 18:32:52: EPOCH 11: training on 609265 raw words (225974 effective words) took 0.9s, 242581 effective words/s\n",
      "INFO - 18:32:53: EPOCH 12: training on 609265 raw words (226203 effective words) took 1.0s, 232220 effective words/s\n",
      "INFO - 18:32:54: EPOCH 13: training on 609265 raw words (226054 effective words) took 1.0s, 234379 effective words/s\n",
      "INFO - 18:32:55: EPOCH 14: training on 609265 raw words (226606 effective words) took 1.0s, 233635 effective words/s\n",
      "INFO - 18:32:56: EPOCH 15: training on 609265 raw words (226342 effective words) took 0.8s, 269917 effective words/s\n",
      "INFO - 18:32:57: EPOCH 16 - PROGRESS: at 87.23% examples, 193658 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 18:32:57: EPOCH 16: training on 609265 raw words (226499 effective words) took 1.1s, 200182 effective words/s\n",
      "INFO - 18:32:58: EPOCH 17 - PROGRESS: at 98.50% examples, 219709 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 18:32:58: EPOCH 17: training on 609265 raw words (226172 effective words) took 1.0s, 222547 effective words/s\n",
      "INFO - 18:32:59: EPOCH 18: training on 609265 raw words (226305 effective words) took 1.0s, 229525 effective words/s\n",
      "INFO - 18:33:00: EPOCH 19: training on 609265 raw words (226281 effective words) took 1.0s, 237102 effective words/s\n",
      "INFO - 18:33:01: EPOCH 20: training on 609265 raw words (225909 effective words) took 0.9s, 245910 effective words/s\n",
      "INFO - 18:33:02: EPOCH 21: training on 609265 raw words (226482 effective words) took 1.0s, 228625 effective words/s\n",
      "INFO - 18:33:03: EPOCH 22: training on 609265 raw words (226151 effective words) took 1.0s, 229994 effective words/s\n",
      "INFO - 18:33:04: EPOCH 23: training on 609265 raw words (226212 effective words) took 1.0s, 228728 effective words/s\n",
      "INFO - 18:33:05: EPOCH 24: training on 609265 raw words (225975 effective words) took 1.0s, 234069 effective words/s\n",
      "INFO - 18:33:06: EPOCH 25: training on 609265 raw words (226061 effective words) took 1.0s, 227186 effective words/s\n",
      "INFO - 18:33:07: EPOCH 26 - PROGRESS: at 98.50% examples, 222298 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 18:33:07: EPOCH 26: training on 609265 raw words (226159 effective words) took 1.0s, 224850 effective words/s\n",
      "INFO - 18:33:08: EPOCH 27: training on 609265 raw words (225900 effective words) took 1.0s, 233542 effective words/s\n",
      "INFO - 18:33:09: EPOCH 28 - PROGRESS: at 98.50% examples, 222527 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 18:33:09: EPOCH 28: training on 609265 raw words (226296 effective words) took 1.0s, 225749 effective words/s\n",
      "INFO - 18:33:10: EPOCH 29: training on 609265 raw words (226062 effective words) took 1.0s, 235309 effective words/s\n",
      "INFO - 18:33:10: Word2Vec lifecycle event {'msg': 'training on 18277950 raw words (6784368 effective words) took 30.2s, 224336 effective words/s', 'datetime': '2023-11-21T18:33:10.183662', 'gensim': '4.3.2', 'python': '3.9.16 (main, Dec  7 2022, 01:12:08) \\n[GCC 11.3.0]', 'platform': 'Linux-5.19.0-45-generic-x86_64-with-glibc2.35', 'event': 'train'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6784368, 18277950)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1f4e783-1cd8-4811-983f-e9b53bc41e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('becky', 0.6330074071884155),\n",
       " ('marge', 0.6316094994544983),\n",
       " ('depressed', 0.6307206153869629),\n",
       " ('unno', 0.603551983833313),\n",
       " ('humiliate', 0.6029792428016663),\n",
       " ('homie', 0.5990012884140015),\n",
       " ('bongo', 0.5930894613265991),\n",
       " ('straighten', 0.5916837453842163),\n",
       " ('ohh', 0.5912268161773682),\n",
       " ('reverend_lovejoy', 0.5878326296806335)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"homer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95ffbd4-ff07-401b-b8a3-3bbde9218fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
